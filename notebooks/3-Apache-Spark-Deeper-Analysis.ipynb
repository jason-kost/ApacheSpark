{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5997ffb-29ef-4ac0-a299-16814b6adc25",
   "metadata": {},
   "source": [
    "# Apache Spark: Deeper Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151cdf24-5f71-4559-a926-f3eb0dd55d41",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook I will perform a deeper analysis of the data in the San Francisco Taxi Trip data set. I will concentrate on trip times, distances, and fares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bbbcc3-b2d9-4320-896d-47eeb7d6917e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "841f6e51-b9c7-4823-878b-a4358989a02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName('test')\n",
    "    .getOrCreate())\n",
    "\n",
    "# The following sets up the ability to output a nicely formatted table.\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.maxNumRows\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7119b579-5e66-453f-b415-bedc0710bec7",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "The following cell reads a Parquet file into a Spark DataFrame and then prints out the schema of the DataFrame. This notebook uses the clean version of the parquet file that was created in the Initial Analysis notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4f27935-897f-44bf-a974-44e724cfc306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- vehicle_placard_number: string (nullable = true)\n",
      " |-- driver_id: string (nullable = true)\n",
      " |-- start_time_local: timestamp (nullable = true)\n",
      " |-- end_time_local: timestamp (nullable = true)\n",
      " |-- pickup_location_latitude: double (nullable = true)\n",
      " |-- pickup_location_longitude: double (nullable = true)\n",
      " |-- pickup_location: string (nullable = true)\n",
      " |-- dropoff_location_latitude: double (nullable = true)\n",
      " |-- dropoff_location_longitude: double (nullable = true)\n",
      " |-- dropoff_location: string (nullable = true)\n",
      " |-- hail_type: string (nullable = true)\n",
      " |-- paratransit: integer (nullable = true)\n",
      " |-- sfo_pickup: integer (nullable = true)\n",
      " |-- qa_flags: string (nullable = true)\n",
      " |-- fare_type: string (nullable = true)\n",
      " |-- meter_fare_amount: double (nullable = true)\n",
      " |-- upfront_pricing: double (nullable = true)\n",
      " |-- promo_rate: double (nullable = true)\n",
      " |-- tolls: double (nullable = true)\n",
      " |-- sf_exit_fee: double (nullable = true)\n",
      " |-- other_fees: double (nullable = true)\n",
      " |-- tip: double (nullable = true)\n",
      " |-- extra_amount: double (nullable = true)\n",
      " |-- total_fare_amount: double (nullable = true)\n",
      " |-- fare_time_milliseconds: integer (nullable = true)\n",
      " |-- trip_distance_meters: double (nullable = true)\n",
      " |-- data_as_of: timestamp (nullable = true)\n",
      " |-- data_loaded_at: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet('/home/jovyan/work/Taxi_Trips_SF_20250621_clean.parquet')\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471bba26-3f8f-4fe3-8563-b8e3c1ab8504",
   "metadata": {},
   "source": [
    "## Create a DataFrame with only the columns that will be used\n",
    "\n",
    "Since there are a limited number of columns that will be used in the analysis let's create a DataFrame that only contains those columns and do any transformations to the data up front, so we won't have to do it every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c9d221b-a955-4418-997d-02eb10c05ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- month: date (nullable = true)\n",
      " |-- total_fare_amount: double (nullable = true)\n",
      " |-- fare_time_milliseconds: integer (nullable = true)\n",
      " |-- fare_time_minutes: double (nullable = true)\n",
      " |-- trip_distance_meters: double (nullable = true)\n",
      " |-- trip_distance_miles: double (nullable = true)\n",
      " |-- sfo_pickup: integer (nullable = true)\n",
      " |-- driver_id: string (nullable = true)\n",
      " |-- vehicle_placard_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "miles_divisor: float = 1609.344\n",
    "\n",
    "df.createOrReplaceTempView('rides')\n",
    "\n",
    "# Note: there are trips with negative fare times. Excluding those.\n",
    "dfs = spark.sql(f\"\"\"\n",
    "select   trunc(to_timestamp(start_time_local, 'y/M/d h:m:s a'), 'month') month\n",
    "         ,total_fare_amount\n",
    "         ,fare_time_milliseconds\n",
    "         ,fare_time_milliseconds / 1000 / 60 as fare_time_minutes\n",
    "         ,trip_distance_meters\n",
    "         ,trip_distance_meters / {miles_divisor} as trip_distance_miles\n",
    "         ,sfo_pickup\n",
    "         ,driver_id\n",
    "         ,vehicle_placard_number\n",
    "from     rides\n",
    "where    fare_time_milliseconds >= 0 -- and (fare_time_milliseconds / 1000 / 60) < 240;\n",
    "\"\"\")\n",
    "\n",
    "# Now let's cache the data\n",
    "\n",
    "dfs.cache()\n",
    "\n",
    "dfs.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7bf9ad-c3fe-42e3-bf14-b369a0ad8f6f",
   "metadata": {},
   "source": [
    "## Create a Temp View on the DataFrame\n",
    "\n",
    "The createOrReplaceTempView() method to create a view on the DataFrame that can be used in SparkSQL to refer to the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c2762b3-23bb-4f74-90e7-b20e75ac3279",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.createOrReplaceTempView('rides')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36d50700-5351-4720-8527-36c7e72b38f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>count</th></tr>\n",
       "<tr><td>4128965</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+\n",
       "|  count|\n",
       "+-------+\n",
       "|4128965|\n",
       "+-------+"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select    count(1) as count\n",
    "from      rides;\n",
    "\"\"\").limit(10) # The limit will cause this to print out nicely formatted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fc7e0f-d781-40f7-827f-1a8670f13c79",
   "metadata": {},
   "source": [
    "## Fare Amount\n",
    "\n",
    "Let's calculate the mininimum, median, ave#rage. and a maximum fare amount for the dataset and then by month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e13e7c3-e95c-44c5-95da-7a70acae2717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>min_amount</th><th>median_amount</th><th>avg_amount</th><th>max_amount</th></tr>\n",
       "<tr><td>0.0</td><td>17.15</td><td>29.16</td><td>55552.25</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+-------------+----------+----------+\n",
       "|min_amount|median_amount|avg_amount|max_amount|\n",
       "+----------+-------------+----------+----------+\n",
       "|       0.0|        17.15|     29.16|  55552.25|\n",
       "+----------+-------------+----------+----------+"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = spark.sql( \"\"\"\n",
    "select min(total_fare_amount) as min_amount\n",
    "       ,median(total_fare_amount) as median_amount\n",
    "       ,round(avg(total_fare_amount), 2) as avg_amount\n",
    "       ,max(total_fare_amount) as max_amount\n",
    "from   rides;\n",
    "\"\"\")\n",
    "\n",
    "df1.limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fe3ab0d-3a77-4c95-b8de-494bf560afb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>month</th><th>min_amount</th><th>median_amount</th><th>avg_amount</th><th>max_amount</th></tr>\n",
       "<tr><td>2022-12-01</td><td>0.0</td><td>17.15</td><td>29.6</td><td>1131.25</td></tr>\n",
       "<tr><td>2023-01-01</td><td>0.0</td><td>17.8</td><td>31.19</td><td>3989.15</td></tr>\n",
       "<tr><td>2023-02-01</td><td>0.0</td><td>17.15</td><td>30.27</td><td>55552.25</td></tr>\n",
       "<tr><td>2023-03-01</td><td>0.0</td><td>17.8</td><td>30.79</td><td>4013.9</td></tr>\n",
       "<tr><td>2023-04-01</td><td>0.0</td><td>17.8</td><td>30.0</td><td>1605.75</td></tr>\n",
       "<tr><td>2023-05-01</td><td>0.0</td><td>17.8</td><td>30.34</td><td>1224.2</td></tr>\n",
       "<tr><td>2023-06-01</td><td>0.0</td><td>17.5</td><td>30.22</td><td>2419.55</td></tr>\n",
       "<tr><td>2023-07-01</td><td>0.0</td><td>17.8</td><td>30.51</td><td>1176.1</td></tr>\n",
       "<tr><td>2023-08-01</td><td>0.0</td><td>17.15</td><td>29.93</td><td>2507.3</td></tr>\n",
       "<tr><td>2023-09-01</td><td>0.0</td><td>17.81</td><td>30.73</td><td>1395.15</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+----------+-------------+----------+----------+\n",
       "|     month|min_amount|median_amount|avg_amount|max_amount|\n",
       "+----------+----------+-------------+----------+----------+\n",
       "|2022-12-01|       0.0|        17.15|      29.6|   1131.25|\n",
       "|2023-01-01|       0.0|         17.8|     31.19|   3989.15|\n",
       "|2023-02-01|       0.0|        17.15|     30.27|  55552.25|\n",
       "|2023-03-01|       0.0|         17.8|     30.79|    4013.9|\n",
       "|2023-04-01|       0.0|         17.8|      30.0|   1605.75|\n",
       "|2023-05-01|       0.0|         17.8|     30.34|    1224.2|\n",
       "|2023-06-01|       0.0|         17.5|     30.22|   2419.55|\n",
       "|2023-07-01|       0.0|         17.8|     30.51|    1176.1|\n",
       "|2023-08-01|       0.0|        17.15|     29.93|    2507.3|\n",
       "|2023-09-01|       0.0|        17.81|     30.73|   1395.15|\n",
       "+----------+----------+-------------+----------+----------+"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = spark.sql( \"\"\"\n",
    "select   month\n",
    "         ,min(total_fare_amount) as min_amount\n",
    "         ,median(total_fare_amount) as median_amount\n",
    "         ,round(avg(total_fare_amount), 2) as avg_amount\n",
    "         ,max(total_fare_amount) as max_amount\n",
    "from     rides\n",
    "group by month\n",
    "order by month;\n",
    "\"\"\")\n",
    "\n",
    "df2.limit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34cb699-8676-40db-b088-63e087d6d52e",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "It seems that people in San Francisco take some really expensive cab rides. In reality, I think this is probably test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1987635-6579-4157-b01f-b6d126e6a923",
   "metadata": {},
   "source": [
    "## Fare Distance\n",
    "\n",
    "Now let's due the same thing, but with the distance. The distance is in meters, so first we will look at it in meters, then we will convert it to miles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c52ee3a-46af-4bc4-b778-89594859e1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>min_distance</th><th>median_distance</th><th>avg_distance</th><th>max_distance</th></tr>\n",
       "<tr><td>0.0</td><td>4184.29</td><td>9728.99</td><td>1165486.9</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------+---------------+------------+------------+\n",
       "|min_distance|median_distance|avg_distance|max_distance|\n",
       "+------------+---------------+------------+------------+\n",
       "|         0.0|        4184.29|     9728.99|   1165486.9|\n",
       "+------------+---------------+------------+------------+"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = spark.sql( \"\"\"\n",
    "select min(trip_distance_meters) as min_distance\n",
    "       ,round(median(trip_distance_meters), 2) as median_distance\n",
    "       ,round(avg(trip_distance_meters), 2) as avg_distance\n",
    "       ,max(trip_distance_meters) as max_distance\n",
    "from   rides;\n",
    "\"\"\")\n",
    "\n",
    "df3.limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3afe0920-ad35-44a9-8c93-c2ebdb1fda2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>month</th><th>min_distance</th><th>median_distance</th><th>avg_distance</th><th>max_distance</th></tr>\n",
       "<tr><td>2022-12-01</td><td>0.0</td><td>4184.3</td><td>9903.68</td><td>429694.8</td></tr>\n",
       "<tr><td>2023-01-01</td><td>0.0</td><td>4506.16</td><td>10593.73</td><td>440316.5</td></tr>\n",
       "<tr><td>2023-02-01</td><td>0.0</td><td>4184.3</td><td>10149.07</td><td>829777.8</td></tr>\n",
       "<tr><td>2023-03-01</td><td>0.0</td><td>4506.2</td><td>10465.37</td><td>1058626.5</td></tr>\n",
       "<tr><td>2023-04-01</td><td>0.0</td><td>4521.29</td><td>10249.52</td><td>554258.1</td></tr>\n",
       "<tr><td>2023-05-01</td><td>0.0</td><td>4553.47</td><td>10401.23</td><td>598676.0</td></tr>\n",
       "<tr><td>2023-06-01</td><td>0.0</td><td>4506.2</td><td>10331.97</td><td>1165486.9</td></tr>\n",
       "<tr><td>2023-07-01</td><td>0.0</td><td>4506.2</td><td>10436.29</td><td>566811.0</td></tr>\n",
       "<tr><td>2023-08-01</td><td>0.0</td><td>4345.2</td><td>10123.31</td><td>726296.9</td></tr>\n",
       "<tr><td>2023-09-01</td><td>0.0</td><td>4506.2</td><td>10449.71</td><td>652267.1</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+------------+---------------+------------+------------+\n",
       "|     month|min_distance|median_distance|avg_distance|max_distance|\n",
       "+----------+------------+---------------+------------+------------+\n",
       "|2022-12-01|         0.0|         4184.3|     9903.68|    429694.8|\n",
       "|2023-01-01|         0.0|        4506.16|    10593.73|    440316.5|\n",
       "|2023-02-01|         0.0|         4184.3|    10149.07|    829777.8|\n",
       "|2023-03-01|         0.0|         4506.2|    10465.37|   1058626.5|\n",
       "|2023-04-01|         0.0|        4521.29|    10249.52|    554258.1|\n",
       "|2023-05-01|         0.0|        4553.47|    10401.23|    598676.0|\n",
       "|2023-06-01|         0.0|         4506.2|    10331.97|   1165486.9|\n",
       "|2023-07-01|         0.0|         4506.2|    10436.29|    566811.0|\n",
       "|2023-08-01|         0.0|         4345.2|    10123.31|    726296.9|\n",
       "|2023-09-01|         0.0|         4506.2|    10449.71|    652267.1|\n",
       "+----------+------------+---------------+------------+------------+"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4 = spark.sql( \"\"\"\n",
    "select month\n",
    "       ,min(trip_distance_meters) as min_distance\n",
    "       ,round(median(trip_distance_meters), 2) as median_distance\n",
    "       ,round(avg(trip_distance_meters), 2) as avg_distance\n",
    "       ,max(trip_distance_meters) as max_distance\n",
    "from   rides\n",
    "group by month\n",
    "order by month;\n",
    "\"\"\")\n",
    "\n",
    "df4.limit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5df1c4b-d18e-4d9e-a1d0-2f967fd5a3a1",
   "metadata": {},
   "source": [
    "### Miles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dcd771f-d7be-4c63-902b-16c2acac3dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>min_distance</th><th>median_distance</th><th>avg_distance</th><th>max_distance</th></tr>\n",
       "<tr><td>0.0</td><td>2.6</td><td>6.05</td><td>724.2</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------+---------------+------------+------------+\n",
       "|min_distance|median_distance|avg_distance|max_distance|\n",
       "+------------+---------------+------------+------------+\n",
       "|         0.0|            2.6|        6.05|       724.2|\n",
       "+------------+---------------+------------+------------+"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5 = spark.sql(\"\"\"\n",
    "select   round(min(trip_distance_miles), 2) as min_distance\n",
    "         ,round(median(trip_distance_miles), 2) as median_distance\n",
    "         ,round(avg(trip_distance_miles), 2) as avg_distance\n",
    "         ,round(max(trip_distance_miles), 2) as max_distance\n",
    "from     rides;\n",
    "\"\"\")\n",
    "\n",
    "df5.limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5f2f17a-fde6-44d2-8564-149efa26aba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>month</th><th>min_distance</th><th>median_distance</th><th>avg_distance</th><th>max_distance</th></tr>\n",
       "<tr><td>2022-12-01</td><td>0.0</td><td>2.6</td><td>6.15</td><td>267.0</td></tr>\n",
       "<tr><td>2023-01-01</td><td>0.0</td><td>2.8</td><td>6.58</td><td>273.6</td></tr>\n",
       "<tr><td>2023-02-01</td><td>0.0</td><td>2.6</td><td>6.31</td><td>515.6</td></tr>\n",
       "<tr><td>2023-03-01</td><td>0.0</td><td>2.8</td><td>6.5</td><td>657.8</td></tr>\n",
       "<tr><td>2023-04-01</td><td>0.0</td><td>2.81</td><td>6.37</td><td>344.4</td></tr>\n",
       "<tr><td>2023-05-01</td><td>0.0</td><td>2.83</td><td>6.46</td><td>372.0</td></tr>\n",
       "<tr><td>2023-06-01</td><td>0.0</td><td>2.8</td><td>6.42</td><td>724.2</td></tr>\n",
       "<tr><td>2023-07-01</td><td>0.0</td><td>2.8</td><td>6.48</td><td>352.2</td></tr>\n",
       "<tr><td>2023-08-01</td><td>0.0</td><td>2.7</td><td>6.29</td><td>451.3</td></tr>\n",
       "<tr><td>2023-09-01</td><td>0.0</td><td>2.8</td><td>6.49</td><td>405.3</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+------------+---------------+------------+------------+\n",
       "|     month|min_distance|median_distance|avg_distance|max_distance|\n",
       "+----------+------------+---------------+------------+------------+\n",
       "|2022-12-01|         0.0|            2.6|        6.15|       267.0|\n",
       "|2023-01-01|         0.0|            2.8|        6.58|       273.6|\n",
       "|2023-02-01|         0.0|            2.6|        6.31|       515.6|\n",
       "|2023-03-01|         0.0|            2.8|         6.5|       657.8|\n",
       "|2023-04-01|         0.0|           2.81|        6.37|       344.4|\n",
       "|2023-05-01|         0.0|           2.83|        6.46|       372.0|\n",
       "|2023-06-01|         0.0|            2.8|        6.42|       724.2|\n",
       "|2023-07-01|         0.0|            2.8|        6.48|       352.2|\n",
       "|2023-08-01|         0.0|            2.7|        6.29|       451.3|\n",
       "|2023-09-01|         0.0|            2.8|        6.49|       405.3|\n",
       "+----------+------------+---------------+------------+------------+"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6 = spark.sql(\"\"\"\n",
    "select month\n",
    "       ,round(min(trip_distance_miles), 2) as min_distance\n",
    "       ,round(median(trip_distance_miles), 2) as median_distance\n",
    "       ,round(avg(trip_distance_miles), 2) as avg_distance\n",
    "       ,round(max(trip_distance_miles), 2) as max_distance\n",
    "from   rides\n",
    "group by month\n",
    "order by month;\n",
    "\"\"\")\n",
    "\n",
    "df6.limit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc38602-4c90-454f-8a15-709bca82967a",
   "metadata": {},
   "source": [
    "## Fare Time\n",
    "\n",
    "Now let's look at how long these trips took."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1f7df3f-bc81-4a48-b6d0-97a4660f45a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>min_time</th><th>median_time</th><th>avg_time</th><th>max_time</th></tr>\n",
       "<tr><td>0</td><td>590602.0</td><td>1261752.33</td><td>2114160592</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+-----------+----------+----------+\n",
       "|min_time|median_time|  avg_time|  max_time|\n",
       "+--------+-----------+----------+----------+\n",
       "|       0|   590602.0|1261752.33|2114160592|\n",
       "+--------+-----------+----------+----------+"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df7 = spark.sql(\"\"\"\n",
    "select   round(min(fare_time_milliseconds), 2) as min_time\n",
    "         ,round(median(fare_time_milliseconds), 2) as median_time\n",
    "         ,round(avg(fare_time_milliseconds), 2) as avg_time\n",
    "         ,round(max(fare_time_milliseconds), 2) as max_time\n",
    "from     rides;\n",
    "\"\"\")\n",
    "\n",
    "df7.limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "345c09b0-042b-413d-a536-ce2438f03ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>month</th><th>min_time</th><th>median_time</th><th>avg_time</th><th>max_time</th></tr>\n",
       "<tr><td>2022-12-01</td><td>0</td><td>633692.5</td><td>848397.11</td><td>463847665</td></tr>\n",
       "<tr><td>2023-01-01</td><td>0</td><td>676269.0</td><td>884829.87</td><td>239690898</td></tr>\n",
       "<tr><td>2023-02-01</td><td>0</td><td>651000.0</td><td>868729.69</td><td>1262603745</td></tr>\n",
       "<tr><td>2023-03-01</td><td>0</td><td>632370.0</td><td>838574.95</td><td>224172633</td></tr>\n",
       "<tr><td>2023-04-01</td><td>0</td><td>570983.0</td><td>771352.03</td><td>237565246</td></tr>\n",
       "<tr><td>2023-05-01</td><td>0</td><td>583804.0</td><td>804657.49</td><td>344215485</td></tr>\n",
       "<tr><td>2023-06-01</td><td>0</td><td>565681.0</td><td>797444.76</td><td>316862037</td></tr>\n",
       "<tr><td>2023-07-01</td><td>0</td><td>571305.0</td><td>797702.93</td><td>241084362</td></tr>\n",
       "<tr><td>2023-08-01</td><td>0</td><td>551061.0</td><td>769072.68</td><td>149564904</td></tr>\n",
       "<tr><td>2023-09-01</td><td>0</td><td>563036.0</td><td>775382.86</td><td>177106505</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+--------+-----------+---------+----------+\n",
       "|     month|min_time|median_time| avg_time|  max_time|\n",
       "+----------+--------+-----------+---------+----------+\n",
       "|2022-12-01|       0|   633692.5|848397.11| 463847665|\n",
       "|2023-01-01|       0|   676269.0|884829.87| 239690898|\n",
       "|2023-02-01|       0|   651000.0|868729.69|1262603745|\n",
       "|2023-03-01|       0|   632370.0|838574.95| 224172633|\n",
       "|2023-04-01|       0|   570983.0|771352.03| 237565246|\n",
       "|2023-05-01|       0|   583804.0|804657.49| 344215485|\n",
       "|2023-06-01|       0|   565681.0|797444.76| 316862037|\n",
       "|2023-07-01|       0|   571305.0|797702.93| 241084362|\n",
       "|2023-08-01|       0|   551061.0|769072.68| 149564904|\n",
       "|2023-09-01|       0|   563036.0|775382.86| 177106505|\n",
       "+----------+--------+-----------+---------+----------+"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df8 = spark.sql(\"\"\"\n",
    "select   month\n",
    "         ,round(min(fare_time_milliseconds), 2) as min_time\n",
    "         ,round(median(fare_time_milliseconds), 2) as median_time\n",
    "         ,round(avg(fare_time_milliseconds), 2) as avg_time\n",
    "         ,round(max(fare_time_milliseconds), 2) as max_time\n",
    "from     rides\n",
    "group by month\n",
    "order by month;\n",
    "\"\"\")\n",
    "\n",
    "df8.limit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b84c38-3805-4550-97f1-aee38ae0a1d3",
   "metadata": {},
   "source": [
    "### Minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e22e085-dbf6-49b1-bfd0-46f33a7552c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>min_time</th><th>median_time</th><th>avg_time</th><th>max_time</th></tr>\n",
       "<tr><td>0.0</td><td>9.84</td><td>21.03</td><td>35236.01</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+-----------+--------+--------+\n",
       "|min_time|median_time|avg_time|max_time|\n",
       "+--------+-----------+--------+--------+\n",
       "|     0.0|       9.84|   21.03|35236.01|\n",
       "+--------+-----------+--------+--------+"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df9 = spark.sql(\"\"\"\n",
    "select   round(min(fare_time_minutes), 2) as min_time\n",
    "         ,round(median(fare_time_minutes), 2) as median_time\n",
    "         ,round(avg(fare_time_minutes), 2) as avg_time\n",
    "         ,round(max(fare_time_minutes), 2) as max_time\n",
    "from     rides;\n",
    "\"\"\")\n",
    "\n",
    "df9.limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0c2aeb4-e1d7-46ab-874d-f3c47dc5db31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>month</th><th>min_time</th><th>median_time</th><th>avg_time</th><th>max_time</th></tr>\n",
       "<tr><td>2022-12-01</td><td>0.0</td><td>10.56</td><td>14.14</td><td>7730.79</td></tr>\n",
       "<tr><td>2023-01-01</td><td>0.0</td><td>11.27</td><td>14.75</td><td>3994.85</td></tr>\n",
       "<tr><td>2023-02-01</td><td>0.0</td><td>10.85</td><td>14.48</td><td>21043.4</td></tr>\n",
       "<tr><td>2023-03-01</td><td>0.0</td><td>10.54</td><td>13.98</td><td>3736.21</td></tr>\n",
       "<tr><td>2023-04-01</td><td>0.0</td><td>9.52</td><td>12.86</td><td>3959.42</td></tr>\n",
       "<tr><td>2023-05-01</td><td>0.0</td><td>9.73</td><td>13.41</td><td>5736.92</td></tr>\n",
       "<tr><td>2023-06-01</td><td>0.0</td><td>9.43</td><td>13.29</td><td>5281.03</td></tr>\n",
       "<tr><td>2023-07-01</td><td>0.0</td><td>9.52</td><td>13.3</td><td>4018.07</td></tr>\n",
       "<tr><td>2023-08-01</td><td>0.0</td><td>9.18</td><td>12.82</td><td>2492.75</td></tr>\n",
       "<tr><td>2023-09-01</td><td>0.0</td><td>9.38</td><td>12.92</td><td>2951.78</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+--------+-----------+--------+--------+\n",
       "|     month|min_time|median_time|avg_time|max_time|\n",
       "+----------+--------+-----------+--------+--------+\n",
       "|2022-12-01|     0.0|      10.56|   14.14| 7730.79|\n",
       "|2023-01-01|     0.0|      11.27|   14.75| 3994.85|\n",
       "|2023-02-01|     0.0|      10.85|   14.48| 21043.4|\n",
       "|2023-03-01|     0.0|      10.54|   13.98| 3736.21|\n",
       "|2023-04-01|     0.0|       9.52|   12.86| 3959.42|\n",
       "|2023-05-01|     0.0|       9.73|   13.41| 5736.92|\n",
       "|2023-06-01|     0.0|       9.43|   13.29| 5281.03|\n",
       "|2023-07-01|     0.0|       9.52|    13.3| 4018.07|\n",
       "|2023-08-01|     0.0|       9.18|   12.82| 2492.75|\n",
       "|2023-09-01|     0.0|       9.38|   12.92| 2951.78|\n",
       "+----------+--------+-----------+--------+--------+"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df10 = spark.sql(\"\"\"\n",
    "select   month\n",
    "         ,round(min(fare_time_minutes), 2) as min_time\n",
    "         ,round(median(fare_time_minutes), 2) as median_time\n",
    "         ,round(avg(fare_time_minutes), 2) as avg_time\n",
    "         ,round(max(fare_time_minutes), 2) as max_time\n",
    "from     rides\n",
    "group by month\n",
    "order by month;\n",
    "\"\"\")\n",
    "\n",
    "df10.limit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932c73a4-0aa3-4d3e-a2e1-cc5b128497ca",
   "metadata": {},
   "source": [
    "## Look at those trips originating at SFO vs. those that don't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bdf64d4-b9f2-46b9-a632-60cf94d9e673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>sfo</th><th>nonsfo</th><th>total</th><th>sfo_percent</th><th>nonsfo_percent</th></tr>\n",
       "<tr><td>1007044</td><td>3121921</td><td>4128965</td><td>24.39</td><td>75.61</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+-------+-------+-----------+--------------+\n",
       "|    sfo| nonsfo|  total|sfo_percent|nonsfo_percent|\n",
       "+-------+-------+-------+-----------+--------------+\n",
       "|1007044|3121921|4128965|      24.39|         75.61|\n",
       "+-------+-------+-------+-----------+--------------+"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df12 = spark.sql(\"\"\"\n",
    "with sfo as (\n",
    "select   *\n",
    "from     rides\n",
    "where    sfo_pickup = 1\n",
    "),\n",
    "nonsfo as (\n",
    "select    *\n",
    "from      rides\n",
    "where     sfo_pickup = 0\n",
    "),\n",
    "total as (\n",
    "select   *\n",
    "from     rides\n",
    "),\n",
    "cnts as (\n",
    "select    (select count(1) from sfo) sfo\n",
    "          , (select count(1) from nonsfo) nonsfo\n",
    "          , (select count(1) from total) total\n",
    ")\n",
    "select   sfo\n",
    "         , nonsfo\n",
    "         , total\n",
    "         , round((sfo / total) * 100, 2) sfo_percent\n",
    "         , round((nonsfo / total) * 100, 2) nonsfo_percent\n",
    "from     cnts\n",
    "\"\"\")\n",
    "\n",
    "df12.limit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc10543-e074-4e9a-84b9-9c553f315f22",
   "metadata": {},
   "source": [
    "## Top Drivers and Top Vehicles\n",
    "\n",
    "Look at the top drivers by total trip time and total miles driven and the top vehicles by total miles driven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81a567cc-98dc-4901-8705-44c3925e6e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>driver_id</th><th>total_trip_time</th></tr>\n",
       "<tr><td>-</td><td>1094813.63</td></tr>\n",
       "<tr><td>F-4919</td><td>458960.04</td></tr>\n",
       "<tr><td>B-6401</td><td>458665.04</td></tr>\n",
       "<tr><td>V-9330</td><td>410371.34</td></tr>\n",
       "<tr><td>A-9919</td><td>377851.54</td></tr>\n",
       "<tr><td>F-9617</td><td>369182.91</td></tr>\n",
       "<tr><td>C-8494</td><td>363182.83</td></tr>\n",
       "<tr><td>U-7473</td><td>343043.12</td></tr>\n",
       "<tr><td>E-4134</td><td>336702.26</td></tr>\n",
       "<tr><td>E-6625</td><td>332399.3</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+---------------+\n",
       "|driver_id|total_trip_time|\n",
       "+---------+---------------+\n",
       "|        -|     1094813.63|\n",
       "|   F-4919|      458960.04|\n",
       "|   B-6401|      458665.04|\n",
       "|   V-9330|      410371.34|\n",
       "|   A-9919|      377851.54|\n",
       "|   F-9617|      369182.91|\n",
       "|   C-8494|      363182.83|\n",
       "|   U-7473|      343043.12|\n",
       "|   E-4134|      336702.26|\n",
       "|   E-6625|       332399.3|\n",
       "+---------+---------------+"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df16 = spark.sql(\"\"\"\n",
    "select   driver_id, round(sum(fare_time_minutes), 2) as total_trip_time\n",
    "from     rides\n",
    "group by driver_id\n",
    "order by total_trip_time desc;\n",
    "\"\"\")\n",
    "\n",
    "df16.limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "565f62b7-65b9-4493-a762-35f04a051d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>driver_id</th><th>trip_distance_miles</th></tr>\n",
       "<tr><td>-</td><td>458162.35</td></tr>\n",
       "<tr><td>U-7473</td><td>95415.4</td></tr>\n",
       "<tr><td>B-6401</td><td>93690.8</td></tr>\n",
       "<tr><td>C-8155</td><td>72712.8</td></tr>\n",
       "<tr><td>A-4301</td><td>71316.52</td></tr>\n",
       "<tr><td>F-9617</td><td>70985.5</td></tr>\n",
       "<tr><td>B-9981</td><td>70238.85</td></tr>\n",
       "<tr><td>F-8012</td><td>68399.6</td></tr>\n",
       "<tr><td>N-7318</td><td>65937.95</td></tr>\n",
       "<tr><td>Y-5184</td><td>65700.91</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+-------------------+\n",
       "|driver_id|trip_distance_miles|\n",
       "+---------+-------------------+\n",
       "|        -|          458162.35|\n",
       "|   U-7473|            95415.4|\n",
       "|   B-6401|            93690.8|\n",
       "|   C-8155|            72712.8|\n",
       "|   A-4301|           71316.52|\n",
       "|   F-9617|            70985.5|\n",
       "|   B-9981|           70238.85|\n",
       "|   F-8012|            68399.6|\n",
       "|   N-7318|           65937.95|\n",
       "|   Y-5184|           65700.91|\n",
       "+---------+-------------------+"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df17  = spark.sql(\"\"\"\n",
    "select   driver_id, round(sum(trip_distance_miles), 2) as trip_distance_miles\n",
    "from     rides\n",
    "group by driver_id\n",
    "order by trip_distance_miles desc;\n",
    "\"\"\")\n",
    "\n",
    "df17.limit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153e3d17-98bd-405e-a08b-83b66ea739b9",
   "metadata": {},
   "source": [
    "### Top N (10) Drivers by Distance by Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b8c2311-f57b-449e-b4ad-9c05b90ec35a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>month</th><th>driver_id</th><th>trip_miles</th></tr>\n",
       "<tr><td>2022-12-01</td><td>-</td><td>18573.0</td></tr>\n",
       "<tr><td>2022-12-01</td><td>A-4301</td><td>5905.0</td></tr>\n",
       "<tr><td>2022-12-01</td><td>A-7414</td><td>4927.0</td></tr>\n",
       "<tr><td>2022-12-01</td><td>B-0575</td><td>4249.0</td></tr>\n",
       "<tr><td>2022-12-01</td><td>F-5183</td><td>4182.0</td></tr>\n",
       "<tr><td>2022-12-01</td><td>B-6401</td><td>3974.0</td></tr>\n",
       "<tr><td>2022-12-01</td><td>C-8155</td><td>3923.0</td></tr>\n",
       "<tr><td>2022-12-01</td><td>F-9617</td><td>3814.0</td></tr>\n",
       "<tr><td>2022-12-01</td><td>B-3671</td><td>3707.0</td></tr>\n",
       "<tr><td>2022-12-01</td><td>F-9314</td><td>3568.0</td></tr>\n",
       "<tr><td>2023-01-01</td><td>-</td><td>160085.0</td></tr>\n",
       "<tr><td>2023-01-01</td><td>F-8012</td><td>5727.0</td></tr>\n",
       "<tr><td>2023-01-01</td><td>A-4301</td><td>5267.0</td></tr>\n",
       "<tr><td>2023-01-01</td><td>V-0006</td><td>4645.0</td></tr>\n",
       "<tr><td>2023-01-01</td><td>A-7414</td><td>4167.0</td></tr>\n",
       "<tr><td>2023-01-01</td><td>B-6401</td><td>4108.0</td></tr>\n",
       "<tr><td>2023-01-01</td><td>B-0575</td><td>4081.0</td></tr>\n",
       "<tr><td>2023-01-01</td><td>C-8155</td><td>4078.0</td></tr>\n",
       "<tr><td>2023-01-01</td><td>B-9981</td><td>4075.0</td></tr>\n",
       "<tr><td>2023-01-01</td><td>D-6085</td><td>3873.0</td></tr>\n",
       "<tr><td>2023-02-01</td><td>-</td><td>86664.0</td></tr>\n",
       "<tr><td>2023-02-01</td><td>F-8012</td><td>5384.0</td></tr>\n",
       "<tr><td>2023-02-01</td><td>B-6401</td><td>4457.0</td></tr>\n",
       "<tr><td>2023-02-01</td><td>A-4523</td><td>4091.0</td></tr>\n",
       "<tr><td>2023-02-01</td><td>C-8155</td><td>4014.0</td></tr>\n",
       "<tr><td>2023-02-01</td><td>B-0575</td><td>3848.0</td></tr>\n",
       "<tr><td>2023-02-01</td><td>U-7473</td><td>3842.0</td></tr>\n",
       "<tr><td>2023-02-01</td><td>B-9981</td><td>3832.0</td></tr>\n",
       "<tr><td>2023-02-01</td><td>V-0006</td><td>3803.0</td></tr>\n",
       "<tr><td>2023-02-01</td><td>A-7414</td><td>3760.0</td></tr>\n",
       "<tr><td>2023-03-01</td><td>-</td><td>16953.0</td></tr>\n",
       "<tr><td>2023-03-01</td><td>A-4301</td><td>5971.0</td></tr>\n",
       "<tr><td>2023-03-01</td><td>U-7473</td><td>5832.0</td></tr>\n",
       "<tr><td>2023-03-01</td><td>B-6401</td><td>5474.0</td></tr>\n",
       "<tr><td>2023-03-01</td><td>F-8012</td><td>4729.0</td></tr>\n",
       "<tr><td>2023-03-01</td><td>A-7414</td><td>4603.0</td></tr>\n",
       "<tr><td>2023-03-01</td><td>B-0575</td><td>4436.0</td></tr>\n",
       "<tr><td>2023-03-01</td><td>A-4523</td><td>4418.0</td></tr>\n",
       "<tr><td>2023-03-01</td><td>C-8155</td><td>4411.0</td></tr>\n",
       "<tr><td>2023-03-01</td><td>N-7487</td><td>4162.0</td></tr>\n",
       "<tr><td>2023-04-01</td><td>-</td><td>16078.0</td></tr>\n",
       "<tr><td>2023-04-01</td><td>C-9504</td><td>8433.0</td></tr>\n",
       "<tr><td>2023-04-01</td><td>N-7318</td><td>5460.0</td></tr>\n",
       "<tr><td>2023-04-01</td><td>A-7414</td><td>5395.0</td></tr>\n",
       "<tr><td>2023-04-01</td><td>A-4301</td><td>5067.0</td></tr>\n",
       "<tr><td>2023-04-01</td><td>B-6401</td><td>5009.0</td></tr>\n",
       "<tr><td>2023-04-01</td><td>D-6085</td><td>4474.0</td></tr>\n",
       "<tr><td>2023-04-01</td><td>B-2690</td><td>4443.0</td></tr>\n",
       "<tr><td>2023-04-01</td><td>U-7473</td><td>4338.0</td></tr>\n",
       "<tr><td>2023-04-01</td><td>Y-9792</td><td>4232.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+---------+----------+\n",
       "|     month|driver_id|trip_miles|\n",
       "+----------+---------+----------+\n",
       "|2022-12-01|        -|   18573.0|\n",
       "|2022-12-01|   A-4301|    5905.0|\n",
       "|2022-12-01|   A-7414|    4927.0|\n",
       "|2022-12-01|   B-0575|    4249.0|\n",
       "|2022-12-01|   F-5183|    4182.0|\n",
       "|2022-12-01|   B-6401|    3974.0|\n",
       "|2022-12-01|   C-8155|    3923.0|\n",
       "|2022-12-01|   F-9617|    3814.0|\n",
       "|2022-12-01|   B-3671|    3707.0|\n",
       "|2022-12-01|   F-9314|    3568.0|\n",
       "|2023-01-01|        -|  160085.0|\n",
       "|2023-01-01|   F-8012|    5727.0|\n",
       "|2023-01-01|   A-4301|    5267.0|\n",
       "|2023-01-01|   V-0006|    4645.0|\n",
       "|2023-01-01|   A-7414|    4167.0|\n",
       "|2023-01-01|   B-6401|    4108.0|\n",
       "|2023-01-01|   B-0575|    4081.0|\n",
       "|2023-01-01|   C-8155|    4078.0|\n",
       "|2023-01-01|   B-9981|    4075.0|\n",
       "|2023-01-01|   D-6085|    3873.0|\n",
       "|2023-02-01|        -|   86664.0|\n",
       "|2023-02-01|   F-8012|    5384.0|\n",
       "|2023-02-01|   B-6401|    4457.0|\n",
       "|2023-02-01|   A-4523|    4091.0|\n",
       "|2023-02-01|   C-8155|    4014.0|\n",
       "|2023-02-01|   B-0575|    3848.0|\n",
       "|2023-02-01|   U-7473|    3842.0|\n",
       "|2023-02-01|   B-9981|    3832.0|\n",
       "|2023-02-01|   V-0006|    3803.0|\n",
       "|2023-02-01|   A-7414|    3760.0|\n",
       "|2023-03-01|        -|   16953.0|\n",
       "|2023-03-01|   A-4301|    5971.0|\n",
       "|2023-03-01|   U-7473|    5832.0|\n",
       "|2023-03-01|   B-6401|    5474.0|\n",
       "|2023-03-01|   F-8012|    4729.0|\n",
       "|2023-03-01|   A-7414|    4603.0|\n",
       "|2023-03-01|   B-0575|    4436.0|\n",
       "|2023-03-01|   A-4523|    4418.0|\n",
       "|2023-03-01|   C-8155|    4411.0|\n",
       "|2023-03-01|   N-7487|    4162.0|\n",
       "|2023-04-01|        -|   16078.0|\n",
       "|2023-04-01|   C-9504|    8433.0|\n",
       "|2023-04-01|   N-7318|    5460.0|\n",
       "|2023-04-01|   A-7414|    5395.0|\n",
       "|2023-04-01|   A-4301|    5067.0|\n",
       "|2023-04-01|   B-6401|    5009.0|\n",
       "|2023-04-01|   D-6085|    4474.0|\n",
       "|2023-04-01|   B-2690|    4443.0|\n",
       "|2023-04-01|   U-7473|    4338.0|\n",
       "|2023-04-01|   Y-9792|    4232.0|\n",
       "+----------+---------+----------+"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df19 = spark.sql(\"\"\"\n",
    "with x as (\n",
    "select   month, driver_id, round(sum(trip_distance_miles), 0) trip_miles\n",
    "from     rides\n",
    "group by month, driver_id\n",
    "order by month, trip_miles desc\n",
    ")\n",
    "select   month, driver_id, trip_miles\n",
    "from (select   *, row_number() over (partition by month order by trip_miles desc) rn\n",
    "from     x) tmp\n",
    "where    rn <= 10;\n",
    "\"\"\")\n",
    "\n",
    "df19.limit(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87f027a9-40ad-450f-9360-3874ae30a970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>vehicle_placard_number</th><th>total_trip_time</th></tr>\n",
       "<tr><td>1273</td><td>507417.77</td></tr>\n",
       "<tr><td>1410</td><td>499777.88</td></tr>\n",
       "<tr><td>1405</td><td>497091.58</td></tr>\n",
       "<tr><td>0134</td><td>466708.3</td></tr>\n",
       "<tr><td>0732</td><td>465961.48</td></tr>\n",
       "<tr><td>1156</td><td>419821.62</td></tr>\n",
       "<tr><td>0784</td><td>419599.12</td></tr>\n",
       "<tr><td>0454</td><td>390736.21</td></tr>\n",
       "<tr><td>1376</td><td>389043.23</td></tr>\n",
       "<tr><td>0251</td><td>377331.29</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------------------+---------------+\n",
       "|vehicle_placard_number|total_trip_time|\n",
       "+----------------------+---------------+\n",
       "|                  1273|      507417.77|\n",
       "|                  1410|      499777.88|\n",
       "|                  1405|      497091.58|\n",
       "|                  0134|       466708.3|\n",
       "|                  0732|      465961.48|\n",
       "|                  1156|      419821.62|\n",
       "|                  0784|      419599.12|\n",
       "|                  0454|      390736.21|\n",
       "|                  1376|      389043.23|\n",
       "|                  0251|      377331.29|\n",
       "+----------------------+---------------+"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df18 = spark.sql(\"\"\"\n",
    "select   vehicle_placard_number, round(sum(fare_time_minutes), 2) as total_trip_time\n",
    "from     rides\n",
    "group by vehicle_placard_number\n",
    "order by total_trip_time desc;\n",
    "\"\"\")\n",
    "\n",
    "df18.limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fd81377-ca4b-4cb3-bd03-636f59df3732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>vehicle_placard_number</th><th>trip_distance_miles</th></tr>\n",
       "<tr><td>0732</td><td>97178.0</td></tr>\n",
       "<tr><td>9010</td><td>93663.8</td></tr>\n",
       "<tr><td>1410</td><td>88764.1</td></tr>\n",
       "<tr><td>0134</td><td>76727.0</td></tr>\n",
       "<tr><td>1555</td><td>75354.9</td></tr>\n",
       "<tr><td>0371</td><td>74055.4</td></tr>\n",
       "<tr><td>1180</td><td>73626.4</td></tr>\n",
       "<tr><td>1420</td><td>73510.7</td></tr>\n",
       "<tr><td>1273</td><td>71949.1</td></tr>\n",
       "<tr><td>0768</td><td>70238.85</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------------------+-------------------+\n",
       "|vehicle_placard_number|trip_distance_miles|\n",
       "+----------------------+-------------------+\n",
       "|                  0732|            97178.0|\n",
       "|                  9010|            93663.8|\n",
       "|                  1410|            88764.1|\n",
       "|                  0134|            76727.0|\n",
       "|                  1555|            75354.9|\n",
       "|                  0371|            74055.4|\n",
       "|                  1180|            73626.4|\n",
       "|                  1420|            73510.7|\n",
       "|                  1273|            71949.1|\n",
       "|                  0768|           70238.85|\n",
       "+----------------------+-------------------+"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df18 = spark.sql(\"\"\"\n",
    "select   vehicle_placard_number, round(sum(trip_distance_miles), 2) as trip_distance_miles\n",
    "from     rides\n",
    "group by vehicle_placard_number\n",
    "order by trip_distance_miles desc;\n",
    "\"\"\")\n",
    "\n",
    "df18.limit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180a2775-e497-4c48-b4c2-8ab27d238b5d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "If you have SQL experience, SparkSQL makes it easy to manipulate data. One thing that I noticed with this data set is that it has some data that look like anamolies. In the next notebook, I will be using SparkML for anamoly detection and time series predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
